{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nflows package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes implemented in the previous task\n",
    "\n",
    "from nflows.transforms import AffineCouplingTransform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try more expressive transforms!\n",
    "\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform   # MAF (from the lecture)\n",
    "from nflows.transforms import PiecewiseRationalQuadraticCouplingTransform  # neural spline flows https://arxiv.org/abs/1906.04032\n",
    "\n",
    "\n",
    "from nflows.transforms import RandomPermutation\n",
    "from nflows.transforms import BatchNorm  # introduced along with the Real-NVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the MNIST dataset with preprocessing\n",
    "class NoisyMNIST(datasets.MNIST):\n",
    "    def __init__(self, *args, noise_std=0.1, subset_digits=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.noise_std = noise_std\n",
    "        self.subset_digits = subset_digits if subset_digits is not None else list(range(10))\n",
    "        self.num_classes = len(self.subset_digits)\n",
    "        self.mapping = {digit: idx for idx, digit in enumerate(self.subset_digits)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        noise = torch.randn_like(img) * self.noise_std\n",
    "        img = img + noise\n",
    "        img = (img - img.mean()) / img.std()  # Standardize the image\n",
    "        mapped_target = torch.tensor(self.mapping.get(target, -1))\n",
    "\n",
    "        if mapped_target.item() < 0:\n",
    "            return img, target, None\n",
    "\n",
    "        one_hot_target = F.one_hot(mapped_target, self.num_classes).float()\n",
    "        return img, target, one_hot_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick your two favorite digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subset of digits to use\n",
    "subset_digits = [2, 5]  # Modify this list to select the desired digits\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = NoisyMNIST(root='./data', train=True, download=True, transform=transform, subset_digits=subset_digits)\n",
    "test_dataset = NoisyMNIST(root='./data', train=False, download=True, transform=transform, subset_digits=subset_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only the desired digits\n",
    "def filter_digits(dataset, digits):\n",
    "    indices = [i for i, (_, target, _) in enumerate(dataset) if target in digits]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = filter_digits(train_dataset, subset_digits)\n",
    "test_dataset = filter_digits(test_dataset, subset_digits)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ...\n",
    "context_features = ... # how do we provide context in this case? look at the data (iterate over train_loader)\n",
    "num_transforms = ...\n",
    "\n",
    "base_dist = StandardNormal(shape=[D])\n",
    "\n",
    "transforms = []\n",
    "\n",
    "for i in range(num_transforms):\n",
    "    transforms.append(RandomPermutation(D))\n",
    "\n",
    "    transforms.append(\n",
    "        MaskedAffineAutoregressiveTransform(\n",
    "            D, hidden_features=...,\n",
    "            context_features=...,\n",
    "            ...\n",
    "        ),\n",
    "    )\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist)\n",
    "\n",
    "print('Number of parameters:', sum(p.numel() for p in flow.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "flow.to(device)\n",
    "\n",
    "optim = AdamW(flow.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "pbar = trange(num_epochs)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in pbar:\n",
    "    flow.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    epoch_pbar = tqdm(train_loader)\n",
    "\n",
    "    for i, batch in enumerate(epoch_pbar):\n",
    "        optim.zero_grad()\n",
    "        imgs, _, context = batch\n",
    "        \n",
    "        # TODO: calculate the loss \n",
    "        ...\n",
    "        loss = ...\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item() \n",
    "\n",
    "        epoch_pbar.set_description(f'current loss: {total_loss/  (i + 1):.2e}')\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    pbar.set_description(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.2e}')\n",
    "\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    flow.eval();\n",
    "\n",
    "    # Sample one image from the flow with some context ([1, 0] in this case)\n",
    "    test_context = ...\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample = flow.sample(1, context=test_context.float().to(device))\n",
    "        img = sample.view(28, 28).cpu().numpy()\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sample several images from the flows with different digits (context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
